import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler, PolynomialFeatures
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.linear_model import Ridge
from sklearn import metrics
from statsmodels.stats.outliers_influence import variance_inflation_factor

# Load dataset
print("Loading dataset...")
df = pd.read_csv('Practice-1 Manufacturing.csv')
print("Dataset loaded. Shape:", df.shape)
print(df.head())

# Calculate Variance Inflation Factor (VIF)
def calc_vif(X):
    print("Calculating VIF...")
    vif = pd.DataFrame()
    vif["VIF"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]
    vif["Feature"] = X.columns
    print("VIF calculation completed.")
    return vif

x = df.drop(columns=['Quality Rating'], errors='ignore')
print(calc_vif(x))

y = df['Quality Rating']

# Train-Test Split
print("Splitting dataset into train and test sets...")
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)
print("Data split completed.")

# Feature Scaling
print("Applying feature scaling...")
scaler = StandardScaler()
x_train_scaled = scaler.fit_transform(x_train)
x_test_scaled = scaler.transform(x_test)
x_scaled = scaler.transform(x)  # Scaling entire dataset
print("Feature scaling applied.")

# Polynomial Features
print("Generating polynomial features...")
poly = PolynomialFeatures(degree=3)
x_train_poly = poly.fit_transform(x_train_scaled)
x_test_poly = poly.transform(x_test_scaled)
x_poly = poly.transform(x_scaled)  # Transform entire dataset
print("Polynomial features generated. Train shape:", x_train_poly.shape, "Test shape:", x_test_poly.shape)

# Model Training (Using Ridge to handle multicollinearity)
print("Training Ridge regression model...")
ridge_model = Ridge(alpha=1.0)
ridge_model.fit(x_train_poly, y_train)
print("Model training completed.")

# Predictions
print("Making predictions...")
y_pred = ridge_model.predict(x_test_poly)
y_poly_pred = ridge_model.predict(x_poly)  # Predicting on whole dataset
print("Predictions completed.")

# Model Evaluation
print("Evaluating model...")
meanAbErr = metrics.mean_absolute_error(y_test, y_pred)
meanSqErr = metrics.mean_squared_error(y_test, y_pred)
rootMeanSqErr = np.sqrt(meanSqErr)
r2_score = metrics.r2_score(y_test, y_pred)

print(f'R squared: {r2_score:.2f}')
print(f'Mean Absolute Error: {meanAbErr}')
print(f'Mean Squared Error: {meanSqErr}')
print(f'Root Mean Squared Error: {rootMeanSqErr}')

# Cross-validation scores
print("Performing cross-validation...")
cv_scores = cross_val_score(ridge_model, x_train_poly, y_train, cv=5, scoring='r2')
print(f'Cross-validation R² scores: {cv_scores}')
print(f'Average CV R² score: {np.mean(cv_scores):.2f}')

# Residual Plot
print("Plotting residual distribution...")
residuals = y_test - y_pred
plt.figure(figsize=(8, 5))
sns.histplot(residuals, kde=True, bins=30)
plt.title("Residual Distribution")
plt.xlabel("Residuals")
plt.show()

# Scatter Plot - Actual vs Predicted
print("Plotting actual vs predicted values...")
plt.scatter(y_test, y_pred, color='blue', alpha=0.6)
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', linewidth=2, linestyle='--')
plt.title("Actual vs Predicted Values")
plt.xlabel("Actual Values")
plt.ylabel("Predicted Values")
plt.show()
print("Visualization completed.")

# Polynomial Regression Curve
print("Plotting polynomial regression curve...")
sorted_idx = np.argsort(x.iloc[:, 0])  # Sorting by first feature
y_poly_pred_sorted = y_poly_pred[sorted_idx]
plt.figure(figsize=(8, 5))
plt.scatter(x.iloc[:, 0], y, color='blue', alpha=0.6, label="Actual Data")
plt.plot(x.iloc[sorted_idx, 0], y_poly_pred_sorted, color='red', linewidth=2, label="Polynomial Curve")
plt.title("Polynomial Regression Curve")
plt.xlabel("Feature")
plt.ylabel("Quality Rating")
plt.legend()
plt.show()
print("Polynomial curve visualization completed.")
